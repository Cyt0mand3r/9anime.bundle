#!/usr/bin/env python

from __builtin__ import getattr, hasattr
from data import Data
import common as Common
from nineanime.jsfdecoder import JSFDecoder
from execjs_eng import JSEngine
import requests
import os


class networking(object):
    def __init__(self):
        self.base_url = Common.BASE_URL
        self.user_agent = Common.USER_AGENT
        self.cache_time = Common.CACHE_TIME
        self.url_cache_dir = Data.url_cache_dir
        self.url_cache_path = Data.data_item_path(Data.url_cache_dir)
        self.req = requests

        Data.ensure_dirs(self.url_cache_path)

    def dir_exists(self, path):
        return os.path.exists(path) and os.path.isdir(path)

    @property
    def file_list(self):
        fp = self.url_cache_path
        return [f for f in Data.list_dir(fp) if Data.file_exists(Data.join_path(fp, f))]

    def datetime_to_utc(self, dt):
        n = Datetime.Now().replace(microsecond=0)
        nutc = Datetime.UTCNow().replace(microsecond=0)
        if n < nutc:
            return dt + (nutc - n)
        elif n == nutc:
            return dt
        return dt - (n - nutc)

    def item_last_modified(self, path, utc=False):
        if os.path.exists(path):
            ts = os.path.getmtime(path)
            if utc:
                return self.datetime_to_utc(Datetime.FromTimestamp(ts)).replace(microsecond=0)
            return Datetime.FromTimestamp(ts).replace(microsecond=0)
        return Datetime.Now().replace(microsecond=0)

    def clear_cache(self, timeout):
        ctime = Datetime.Now()
        count = 0
        #Log.Debug("* Clearing '{}' items older than {}".format(self.url_cache_dir, str(ctime - timeout)))
        for fn in self.file_list:
            fp = os.path.join(self.url_cache_path, fn)
            if (self.item_last_modified(fp) + timeout) <= ctime:
                if self.dir_exists(fp):
                    continue
                Data.Remove(os.path.join(self.url_cache_dir, fn))
                count += 1
        if (count > 0):
            Log.Debug('* Cleaned {} Cached files from {}'.format(count, self.url_cache_dir))
        return

    def HTTPRequest(self, url, hname, headers=None, cacheTime=None, values=None, follow_redirects=True, method='GET', verify=True, count=0):
        if not headers:
            headers = {'Referer': url, 'User-Agent': self.user_agent}
        if not 'User-Agent' in headers.keys():
            headers['User-Agent'] = self.user_agent

        nc = headers.get('Cookie', None)
        cookies = dict((k,v) for (k,v) in Regex(r'(\w+)\=([^\;]+)').findall(nc)) if nc else {}

        if values:
            method = 'POST'
        rm = getattr(self.req, method.lower())
        try:
            res = rm(url, headers=headers, cookies=cookies, data=values, verify=verify, allow_redirects=follow_redirects)
            res.raise_for_status()
            Log.Debug("* Caching '{}' to {}".format(url, self.url_cache_dir))
            Data.Save(Data.HTTP(hname), res, True)
            return res
        except Exception as ee:
            if 'ssl' in str(ee) and (count == 0):
                res = self.HTTPRequest(url, hname, headers, cacheTime, values, follow_redirects, method, False, 1)
                return res
            Log.Error(u"* <networking.HTTPRequest[error]>: Cannot handle '{}'".format(url))
            Log.Error(u"{}".format(ee))
            raise Ex.ContextException("9anime HTTPRequest Failed. Contact Twoure with Log files.")

    def Request(self, url, headers=None, cacheTime=None, values=None, follow_redirects=True, method='GET'):
        if not cacheTime:
            cacheTime = int(self.cache_time.total_seconds())
        timeout = Datetime.Delta(seconds=cacheTime)
        self.clear_cache(timeout)
        res = None
        hn = Hash.MD5(url)

        if (len([fn for fn in self.file_list if fn == hn]) == 0):
            return self.HTTPRequest(url, hn, headers, cacheTime, values, follow_redirects, method)

        for fn in self.file_list:
            if fn != hn:
                continue
            fp = os.path.join(self.url_cache_path, fn)
            if (self.item_last_modified(fp) + timeout) <= Datetime.Now():
                Log.Debug("* Re-Caching '{}' to {}".format(url, self.url_cache_dir))
                return self.HTTPRequest(url, hn, headers, cacheTime, values, follow_redirects, method)
            else:
                Log.Debug("* Fetching '{}' from {}".format(url, self.url_cache_dir))
                return Data.Load(Data.HTTP(fn), True)
        raise Ex.MediaNotAvailable

    def ElementFromURL(self, url, headers=None, cacheTime=None, values=None, follow_redirects=True, method='GET'):
        res = self.Request(url, headers, cacheTime, values, follow_redirects, method)
        return HTML.ElementFromString(res.content)

    def get_cookies(self):
        headers = {'Referer': self.base_url, 'User-Agent': self.user_agent}
        tok_url = self.base_url+'/token'
        try:
            res = self.Request(tok_url, headers)
            c = dict()
            for cs in [ v.split(';')[0] for v in res.headers.values() if v.startswith('__cfduid') ]:
                kk,vv = cs.split('=')
                c.update({kk:vv})
            c = dict((k, res.cookies[k]) for k in res.cookies.keys()) if hasattr(res, 'cookies') and (len(c) == 0) else c
            try:
                t = JSFDecoder(res.content).ca_decode()
                if 'function' in t:
                    raise ValueError("JSFDecoder: No JSF code found")
            except Exception as e:
                Log.Debug('JSFDecoder failed, falling back to exejs >>> {}'.format(e))
                try:
                    tok = res.content.decode('utf-8')
                    sep = Regex(ur'(?u)\S\[\S\]\=.+?\(\S\)([\,\;])').search(tok).group(1)
                    tok = Regex(ur'(?u)((\S\[\S\])\=.+?\(\S\))').sub(ur'\1%sjssuckit.push(\2)' %sep, tok)
                    def re_rep(string, search, rep):
                        return string.replace(search, rep)
                    tok = Regex(ur'(?u)jssuckit\.push\((.+?)\)').sub( lambda m: re_rep(tok, m.group(1), 'document["cookie"]'), tok)
                    d = D(
                        "dmFyIGpzc3Vja2l0ID0gW107IHZhciB3aW5kb3cgPSB0aGlzOyB0aGlz"
                        "LmRvY3VtZW50ID0ge307IHRoaXMubG9jYXRpb24gPSAiJXMiOyAlczs_"
                        ) %(self.base_url, tok)
                    ctx = JSEngine.compile(d.encode('utf-8'))
                    t = ctx.exec_('return jssuckit')
                except Exception as e:
                    Log.Exception("JRE Failed: Clear cache and try again. >>>")
                    self.unCache(tok_url)
                    return ''
            for cook in t:
                for (k, v) in Regex(r'(\w+)\=([^\;]+)').findall(cook):
                    if (k == 'expires') or (k == 'path'):
                        continue
                    elif (k in c) and (v in c.values()):
                        continue
                    c.update({k:v})
            return '; '.join([ '{}={}'.format(k,c[k]) for k in c.keys() ])
        except:
            Log.Exception("* <network.Request[error]>: Cannot handle token cookie >>>")
        return ''

    def cookies_from_res(self, res):
        return '; '.join([ str(s).split('Cookie ')[1].rsplit(' for ')[0] for s in res.cookies if ('Cookie ' in str(s)) ])

    def unCache(self, url):
        Data.Remove(Data.HTTP(Hash.MD5(url)))

Network = networking()
